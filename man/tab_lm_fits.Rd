% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tab_lm_fits.R
\name{tab_lm_fits}
\alias{tab_lm_fits}
\title{APA: flextable for Comparing the Performance of Linear models}
\usage{
tab_lm_fits(
  x,
  caption = "Comparison of Linear Model Performane Metrics",
  general_note = NA,
  sort = c("AIC", "R2_adjusted"),
  d = 2
)
}
\arguments{
\item{x}{REQUIRED: List. at least 2 lm models, bare names, If named list, then names appear in the table}

\item{caption}{Optional: Text. Caption for the table}

\item{general_note}{Optional: Text. General note for footer of APA table}

\item{sort}{Optional: metrics to sort by, default = c("AIC", "R2_adjusted"), may include: "AIC", "BIC", "R2", "R2_adjusted", "RMSE"}

\item{d}{Optional: Number. Digits after the decimal place}
}
\value{
a flextable object
}
\description{
Create a flextable for Comparing the Performance of Linear models via Several Metrics
}
\details{
Model performance metrics

In regression model, the most commonly known evaluation metrics include:
\itemize{
\item \strong{Akaike's Information Criteria (AIC)} is a metric developed by the Japanese Statistician, Hirotugu Akaike, 1970. The basic idea of AIC is to penalize the inclusion of additional variables to a model. It adds a penalty that increases the error when including additional terms. The \strong{lower} the AIC, the better the model.
\item \strong{Bayesian information criteria (BIC)} is a variant of AIC with a stronger penalty for including additional variables to the model. The basic idea of AIC is to penalize the inclusion of additional variables to a model. It adds a penalty that increases the error when including additional terms. The \strong{lower} the BIC, the better the model.
\item \strong{R-squared (R2)}, which is the proportion of variation in the outcome that is explained by the predictor variables. In multiple regression models, R2 corresponds to the squared correlation between the observed outcome values and the predicted values by the model. The \strong{Higher} the R-squared, the better the model.
\item \strong{Adjusted R-squared (adj-R2)} adjusts the R2 for having too many variables in the model.  \strong{Larger} values are better.
\item \strong{Root Mean Squared Error (RMSE)}, which measures the average error performed by the model in predicting the outcome for an observation. Mathematically, the RMSE is the square root of the mean squared error (MSE), which is the average squared difference between the observed actual outcome values and the values predicted by the model. So, MSE = mean((observed - predicted)^2) and RMSE = sqrt(MSE). The \strong{lower} the RMSE, the better the model.
}

Including additional variables in the model will \strong{always} increase the R2 and reduce the RMSE.  Conversely, AIC, BIC, and adjusted-R2 penalize for model complexity and are more commonly used for model evaluation and selection, as these are unbiased estimated fo the model prediction error, and thus should be the basis of model comparison and optimal model selection.
Note: regression metrics are all internal measures, that is they have been computed on the \strong{same data} that was used to build the regression model. They tell you how well the model fits to the data in hand.
}
\examples{

m1 <- lm(dist ~ 1, cars)
m2 <- lm(dist ~ speed, cars)
tab_lm_fits(list("null" = m1, "main" = m2))

}
